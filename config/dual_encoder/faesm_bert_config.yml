loss:
  loss_type: CLIPLoss
  loss_args: {}

model:
  model_type: lora_wrap.LoRAWrapper
  model_args:
    base_model_name: faesm_bert.FAEsmBertEncoder
    base_model_args:
      bert_model_name: prajjwal1/bert-small
      esm_model_name: facebook/esm2_t12_35M_UR50D

optimizer:
  optimizer_type: Adam
  optimizer_args:
    desc:
      lr: 0.05
    prot:
      lr: 0.05
    other:
      lr: 0.01

scheduler:
  scheduler_type: ReduceLROnPlateau
  scheduler_args:
    mode: min
    factor: 0.5
    patience: 5
    threshold: 0.0001
    threshold_mode: abs

dataset:
  dataset_type: RawDescSeqDataset
  dataset_args:
    train:
      adata_path: raw_data/adata/human_adata.h5ad
      desc_path: raw_data/descriptions/train_desc.json
      use_ratio: 1.
    val:
      adata_path: raw_data/adata/human_adata.h5ad
      desc_path: raw_data/descriptions/val_desc.json
      use_ratio: 1.

dataloader:
  batch_size: 16
  collate_fn: dual_encoder_collate_fn
  collate_args:
    seq_tokenizer_args:
      pretrained_model_name_or_path: facebook/esm2_t12_35M_UR50D #facebook/esm2_t30_150M_UR50D
    desc_tokenizer_args:
      pretrained_model_name_or_path: prajjwal1/bert-small
  sampler_type: RandomSampler
  sampler_args: {}

evaluator:
  metric_type: TopKAcc
  metric_args:
    ks: [10, 20, 50]

# logger:
#   logger_type: WandbLogger
#   logger_args:
#     project: NL2UniProt
#     name: FAESM25M-SmallBERT-lora-training
#     offline: False

# save_model:
#   save_model: True
#   mode: max
#   monitor: val/Avg_TopKAcc
#   save_model_path: trained_models/faesm25m_smallbert_lora

trainer:
  max_epochs: 50
  devices: [0]
  enable_progress_bar: True
  precision: bf16
  # resume_from_checkpoint: trained_models/esm150m_smallbert_fae/model-epoch=00.ckpt
  # precision: 32
  # strategy: ddp
  # resume_from_checkpoint: None


pl_module: DualEncoder