loss:
  loss_type: CLIPLoss
  loss_args: {}

model:
  model_type: esm_bert.EsmBertEncoder
  model_args:
    bert_model_name: prajjwal1/bert-small
    esm_model_name: facebook/esm2_t12_35M_UR50D
    num_unfrozen_bert_layers: 4
    num_unfrozen_esm_layers: 4
    # init_method: xavier

optimizer:
  optimizer_type: Adam
  optimizer_args:
    desc:
      lr: 0.00005
    prot:
      lr: 0.00005
    other:
      lr: 0.001

scheduler:
  scheduler_type: ReduceLROnPlateau
  scheduler_args:
    mode: min
    factor: 0.5
    patience: 5
    threshold: 0.0001
    threshold_mode: abs

dataset:
  dataset_type: RawDescSeqDataset
  dataset_args:
    train:
      data_path: raw_data/fasta_files/latest_human_seqs.fasta
      desc_path: raw_data/misc/train_descriptions.json
      use_ratio: 1.0
    val:
      data_path: raw_data/fasta_files/latest_human_seqs.fasta
      desc_path: raw_data/misc/val_descriptions.json
      use_ratio: 1.0

dataloader:
  batch_size: 32
  num_workers: 4
  collate_fn: dual_encoder_collate_fn
  collate_args:
    seq_tokenizer_args:
      pretrained_model_name_or_path: facebook/esm2_t12_35M_UR50D #facebook/esm2_t30_150M_UR50D
    desc_tokenizer_args:
      pretrained_model_name_or_path: prajjwal1/bert-small
  sampler_type: RandomSampler #DistributedSampler
  sampler_args: {}

evaluator:
  metric_type: TopKAcc
  metric_args:
    ks: [20, 50, 100]

logger:
  logger_type: wandb
  logger_args: #{}
    project: NL2UniProt
    name: 4x-mixed-data-esm25m-smallbert
    # group: DDP
    # offline: False

save_model:
  save_model: true
  mode: min
  monitor: val/epoch_loss
  save_model_dir: trained_models/esm_bert #esm25m_4layer_smallbert
  identifier: 4x_mixed_data_esm25m_smallbert

trainer:
  trainer_type: clip_trainer.CLIPTrainer
  trainer_args:
    max_epochs: 50
    # device: cuda:0
    enable_progress_bar: false
    use_amp: true
    # resume_from_checkpoint: trained_models/2024-12-23-03-14-25-test/state.pt
    # precision: 32
    # strategy: ddp
    # resume_from_checkpoint: None